{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook\n",
    "\n",
    "## Motivation.\n",
    "### The datasets\n",
    "Our data consists of 4 diffent datasets that describes the counties across the united states. Our final datasets contain 14 variables after cleaning and preprocessing and to name a few it contains adult obesity, mean income, poltical stance etc.\n",
    "\n",
    "### Why health, Fastfood chains & income data?\n",
    "One of the problems of some modern welfare states is a tendency of obesity. \n",
    "\n",
    "We have choosen health in the US because we would like to study how other social factors may have an impact on ones health. The Health data allows us to investigate many potential factors in determining obesity in the United States of America, these factors are: Income, exposure to fastfood restaurants, physical health, mental health, smoking habits, drinking habits, employment status and political orientation. \n",
    "\n",
    "The Fastfood chain data can also have an effect on the health. The trend seems that the Americans every year spend more money on take-away excluding 2020, however that year was also extraordinary in regards to lockdown caused by COVID-19. And the income data is just as relevant, as sources tells us that almost the same percentage of the American income is spend on take-away, where the percentage spend on homemade food is decreasing.\n",
    "\n",
    "#### The idea and goal of the project \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "* Write about your choices in data cleaning and preprocessing\n",
    "* Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis.\n",
    "\n",
    "### Choices in data cleaning and preprocessing\n",
    "\n",
    "#### County Health Rankings Dataset\n",
    "The Health Dataset consists of 3193 rows and 250 columns. A row corresponds to a county in the US and the first columns consists of a FIPS code, the name of the state the county is within and the name of the county. The rest of the columns describe different health factors of each county such as obesity, smokers, alcoholism, education etc.\n",
    "\n",
    "226 of the rows in the data has an x in a column named \"Unreliable\". The column is not further explained in the data description given in the [PDF of data description](https://www.countyhealthrankings.org/sites/default/files/media/document/DataDictionary_2020_2.pdf) but for the sake of the column name, these rows will be removed. \n",
    "\n",
    "Due to the way pandas can read a csv, the first zero of the FIPS code can be automatically omitted. This will not allow `plotly` to plot those states, which is why we need to apply a zero infront of the row if the number is less than 5 digits short using:\n",
    "\n",
    "`df['FIPS']=df'FIPS'].apply(lambda x: '{0:0>5}'.format(x))`\n",
    "\n",
    "Too easier combine the different datasets, a dictionary of the states and their abbreviation (`us_state_to_abbrev`) is needed to translate the states. This allows us to use `pandas.groupby` to combine the states and counties. It is important to groupby both State and County since some county names may repeat across different States.\n",
    "\n",
    "A few rows also had floats represented as a string, which had to be translated into floats to analyse.\n",
    "\n",
    "#### FastFood Chains across America\n",
    "The FastFood Dataset consists of 10000 rows and 14 columns. A row corresponds to a resturant in the US and the first columns consists of a address, the name of the Fastfood chain, the state etc. This dataset does not have a column for county, so we have to extract that information ourselves and create a new column to group this data together with the other datasets. Since the postalcode is a column in the dataset, we can use `pgeocode` to extract the county information for each resturant. \n",
    "\n",
    "```\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "county_names = []\n",
    "for i in range(len(FastFood)):\n",
    "    county_names.append(nomi.query_postal_code(FastFood[\"postalCode\"][i]).county_name)\n",
    "    \n",
    "FastFood[\"County\"] = county_names\n",
    "```\n",
    "\n",
    "The focus variable we are interested in from this data set is the a count of how many chains there is in each county as well as what fastfood chains we can see across the states.\n",
    "\n",
    "#### US Household Income Statistics & Political data\n",
    "\n",
    "The US Household Dataset consists of 32526 rows and 19 columns. Each row corresponds to some area code within a county. This and the Political data has the word *County* added to each string value in the `County` column, meaning we need to remove the last word of each element in this column. The focus variable we are interested in from the income data set is the `Mean` column which represents the mean income for households in that county and per_gop from the Political data.\n",
    "\n",
    "#### Data Cleaning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "health = pd.read_csv(\"../Datasets/rankmd.csv\", delimiter=\";\")\n",
    "FastFood = pd.read_csv(\"../Datasets/FastFoodRestaurants.csv\")\n",
    "income = pd.read_csv(\"../Datasets/kaggle_income.csv\", encoding=\"ISO 8859-1\")\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "    \n",
    "# Inverting the dictionary\n",
    "abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n",
    "\n",
    "# Creating a state dataset\n",
    "FastFood['State'] = FastFood['province'].map(abbrev_to_us_state)\n",
    "States = health.copy()\n",
    "States = States[States['FIPS'].astype(str).str.endswith('000')]\n",
    "\n",
    "# Converting FIPS to string\n",
    "health['FIPS']=health['FIPS'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "\n",
    "#Setting the food_enviornment index as float instead of string\n",
    "health[\"food_environment_index_Food Environment Index\"] = health[\"food_environment_index_Food Environment Index\"].str.replace(\",\",\".\").astype(float)\n",
    "\n",
    "# Removing ' County' from the county names in income\n",
    "income[\"County\"] = income.County.str.replace(' County', '')\n",
    "\n",
    "# Merging income and health data\n",
    "temp_df = income.groupby([\"State_Name\",\"County\"]).mean().reset_index()\n",
    "new_df = pd.merge(health.copy(), temp_df.copy(),  how='left', left_on=['State','County'], right_on = ['State_Name','County'])\n",
    "\n",
    "#ONLY NEEDS TO BE RAN ONCE AS THE COUNTIES ARE STORED IN THE CSV.\n",
    "\n",
    "#!{sys.executable} -m pip install pgeocode\n",
    "#import pgeocode\n",
    "\n",
    "#nomi = pgeocode.Nominatim('us')\n",
    "#county_names = []\n",
    "#for i in range(len(FastFood)):\n",
    "#    county_names.append(nomi.query_postal_code(FastFood[\"postalCode\"][i]).county_name)\n",
    "    \n",
    "#FastFood[\"County\"] = county_names\n",
    "#FastFood.to_csv(\"../Datasets/FastFoodRestaurants.csv\")\n",
    "\n",
    "# Merging fastfood data with income and health data\n",
    "temptemp = FastFood.groupby([\"State\", \"County\"]).count().reset_index()[['State','County','address']]\n",
    "tempo = temptemp.rename(columns={'address':'nr of FFchains'})\n",
    "data_df = pd.merge(new_df, tempo,  how='left', left_on=['State','County'], right_on =['State','County'])\n",
    "data_df['nr of FFchains'] = data_df['nr of FFchains'].fillna(0)\n",
    "\n",
    "# Importing political data\n",
    "poldata = pd.read_csv(\"../Datasets/2020_US_County_Level_Presidential_Results.csv\", delimiter=\",\")\n",
    "\n",
    "# Removing ' County' from the county names\n",
    "poldata[\"county_name\"] = poldata.county_name.str.replace(' County', '')\n",
    "\n",
    "# Merging the political data with the other data\n",
    "merged=pd.merge(data_df.copy(), poldata.copy(),  how='left', left_on=['State','County'], right_on = ['state_name','county_name'])\n",
    "\n",
    "# Snipping the columns to a more clean dataset\n",
    "data = merged[[\"premature_deathYears_of_Potential_Life_Lost_Rate\",'adult_obesity_% Adults with Obesity',\n",
    "                \"adult_smoking_% Smokers\", \"excessive_drinking_% Excessive Drinking\", \"food_environment_index_Food Environment Index\",\n",
    "                \"uninsured_% Uninsured\", \"unemployed_% Unemployed\", 'nr of FFchains', 'Mean',\n",
    "                \"poor_physical_health_days_Average Number of Physically Unhealthy Days\",\n",
    "                \"poor_mental_health_days_Average Number of Mentally Unhealthy Days\",\"per_gop\"]]\n",
    "\n",
    "# Dropping NaNs\n",
    "data = data.dropna()\n",
    "\n",
    "# Creating a response value to predict a ML model\n",
    "data['is_obese'] = data['adult_obesity_% Adults with Obesity']>=33\n",
    "data = data.drop(['adult_obesity_% Adults with Obesity', 'Mean'],axis=1)\n",
    "\n",
    "# Cleansing columns, making unemployed percentage an integer and physical and mental unhealthy days floats\n",
    "data[\"unemployed_% Unemployed\"] = data[\"unemployed_% Unemployed\"].str.replace(\",\",\".\").astype(float).astype(int)\n",
    "data[\"poor_physical_health_days_Average Number of Physically Unhealthy Days\"] = data[\"poor_physical_health_days_Average Number of Physically Unhealthy Days\"].str.replace(\",\",\".\").astype(float)\n",
    "data[\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\"] = data[\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\"].str.replace(\",\",\".\").astype(float)\n",
    "\n",
    "# Storing the merged, cleansed dataset as a csv file\n",
    "data.to_csv(\"../Datasets/Mixed_data.csv\")\n",
    "merged.to_csv(\"../Datasets/All_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis.\n",
    "* Describe your data analysis and explain what you've learned about the dataset. *If relevant, talk about your machine-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre.\n",
    "* Which genre of data story did you use?\n",
    "* Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?\n",
    "* Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations.\n",
    "* Explain the visualizations you've chosen.\n",
    "* Why are they right for the story you want to tell?\n",
    "\n",
    "### Folium map\n",
    "\n",
    "### Choroploth Map\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think critically about your creation\n",
    "* What went well?\n",
    "* What is still missing? What could be improved? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
