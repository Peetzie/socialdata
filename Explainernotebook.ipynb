{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook\n",
    "\n",
    "## Motivation.\n",
    "### The datasets\n",
    "Our data consists of 4 diffent datasets that describes the counties across the united states. Our final datasets contain 14 variables after cleaning and preprocessing and to name a few it contains adult obesity, mean income, poltical stance etc.\n",
    "\n",
    "### Why health, Fastfood chains & income data?\n",
    "One of the problems of some modern welfare states is a tendency of obesity. \n",
    "\n",
    "We have choosen health in the US because we would like to study how other social factors may have an impact on ones health. The Health data allows us to investigate many potential factors in determining obesity in the United States of America, these factors are: Income, exposure to fastfood restaurants, physical health, mental health, smoking habits, drinking habits, employment status and political orientation. \n",
    "\n",
    "The Fastfood chain data can also have an effect on the health. The trend seems that the Americans every year spend more money on take-away excluding 2020, however that year was also extraordinary in regards to lockdown caused by COVID-19. And the income data is just as relevant, as sources tells us that almost the same percentage of the American income is spend on take-away, where the percentage spend on homemade food is decreasing.\n",
    "\n",
    "#### The idea and goal of the project \n",
    "\n",
    "The idea arose from the green challange and the third SDG: Good health and well being. With data from such as grand nation that is known to have such a large range in quality of health culture, the idea of exploring their health arose, and would hopefully give us an insight into if different changes to society could have an effect on the individuals health. \n",
    "\n",
    "The goal of the project is to investigate if the society around you and factors of your person and common area has anything to do with your own health. To investigate whether or not living in societies with lower income, different political orientation or other factors determines how healthy you on average are yourself. In the end, we wanted to tell a story that showcases american health behaviour and wether or not these different aspects of american life can be put into a category, to tell us if we can predict these unhealthy counties based on many other variables about the common american located differently across the US. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "* Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "### Choices in data cleaning and preprocessing\n",
    "\n",
    "#### County Health Rankings Dataset\n",
    "The Health Dataset consists of 3193 rows and 250 columns. A row corresponds to a county in the US and the first columns consists of a FIPS code, the name of the state the county is within and the name of the county. The rest of the columns describe different health factors of each county such as obesity, smokers, alcoholism, education etc.\n",
    "\n",
    "226 of the rows in the data has an x in a column named \"Unreliable\". The column is not further explained in the data description given in the [PDF of data description](https://www.countyhealthrankings.org/sites/default/files/media/document/DataDictionary_2020_2.pdf) but for the sake of the column name, these rows will be removed. \n",
    "\n",
    "Due to the way pandas can read a csv, the first zero of the FIPS code can be automatically omitted. This will not allow `plotly` to plot those states, which is why we need to apply a zero infront of the row if the number is less than 5 digits short using:\n",
    "\n",
    "`df['FIPS']=df'FIPS'].apply(lambda x: '{0:0>5}'.format(x))`\n",
    "\n",
    "Too easier combine the different datasets, a dictionary of the states and their abbreviation (`us_state_to_abbrev`) is needed to translate the states. This allows us to use `pandas.groupby` to combine the states and counties. It is important to groupby both State and County since some county names may repeat across different States.\n",
    "\n",
    "A few rows also had floats represented as a string, which had to be translated into floats to analyse.\n",
    "\n",
    "#### FastFood Chains across America\n",
    "The FastFood Dataset consists of 10000 rows and 14 columns. A row corresponds to a resturant in the US and the first columns consists of a address, the name of the Fastfood chain, the state etc. This dataset does not have a column for county, so we have to extract that information ourselves and create a new column to group this data together with the other datasets. Since the postalcode is a column in the dataset, we can use `pgeocode` to extract the county information for each resturant. \n",
    "\n",
    "```\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "county_names = []\n",
    "for i in range(len(FastFood)):\n",
    "    county_names.append(nomi.query_postal_code(FastFood[\"postalCode\"][i]).county_name)\n",
    "    \n",
    "FastFood[\"County\"] = county_names\n",
    "```\n",
    "\n",
    "The focus variable we are interested in from this data set is the count of how many restaurants there is in each county as well as what fastfood chains we can see across the states.\n",
    "\n",
    "#### US Household Income Statistics & Political data\n",
    "\n",
    "The US Household Dataset consists of 32526 rows and 19 columns. Each row corresponds to some area code within a county. This and the Political data has the word *County* added to each string value in the `County` column, meaning we need to remove the last word of each element in this column. The focus variable we are interested in from the income data set is the `Mean` column which represents the mean income for households in that county and per_gop from the Political data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Code\n",
    "##### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sys\n",
    "#!{sys.executable} -m pip install folium\n",
    "import folium\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import os\n",
    "#!{sys.executable} -m pip install geopandas\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "!{sys.executable} -m pip install colorcet\n",
    "import colorcet as cc\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading datasets\n",
    "health = pd.read_csv(\"Datasets/rankmd.csv\", delimiter=\";\")\n",
    "FastFood = pd.read_csv(\"Datasets/FastFoodRestaurants.csv\")\n",
    "income = pd.read_csv(\"Datasets/kaggle_income.csv\", encoding=\"ISO 8859-1\")\n",
    "poldata = pd.read_csv(\"Datasets/2020_US_County_Level_Presidential_Results.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning up the data formatting\n",
    "We added abbreviations for each state to the data set, likewise we cleaned it up by ensuring that the formatting was correct. I.e strings should be strings, floats; floats and so on. \n",
    "As our data consists of 4 dataset we also merged them into some more *handy* datasets that was easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of the states to abbreviation\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "    \n",
    "# Inverting the dictionary\n",
    "abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n",
    "\n",
    "# Creating a state dataset\n",
    "FastFood['State'] = FastFood['province'].map(abbrev_to_us_state)\n",
    "States = health.copy()\n",
    "States = States[States['FIPS'].astype(str).str.endswith('000')]\n",
    "\n",
    "# Converting FIPS to string\n",
    "health['FIPS']=health['FIPS'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "\n",
    "#Setting the food_enviornment index as float instead of string\n",
    "health[\"food_environment_index_Food Environment Index\"] = health[\"food_environment_index_Food Environment Index\"].str.replace(\",\",\".\").astype(float)\n",
    "\n",
    "# Removing ' County' from the county names in income\n",
    "income[\"County\"] = income.County.str.replace(' County', '')\n",
    "\n",
    "# Merging income and health data\n",
    "temp_df = income.groupby([\"State_Name\",\"County\"]).mean().reset_index()\n",
    "new_df = pd.merge(health.copy(), temp_df.copy(),  how='left', left_on=['State','County'], right_on = ['State_Name','County'])\n",
    "\n",
    "#ONLY NEEDS TO BE RAN ONCE AS THE COUNTIES ARE STORED IN THE CSV.\n",
    "\n",
    "#!{sys.executable} -m pip install pgeocode\n",
    "#import pgeocode\n",
    "\n",
    "#nomi = pgeocode.Nominatim('us')\n",
    "#county_names = []\n",
    "#for i in range(len(FastFood)):\n",
    "#    county_names.append(nomi.query_postal_code(FastFood[\"postalCode\"][i]).county_name)\n",
    "    \n",
    "#FastFood[\"County\"] = county_names\n",
    "#FastFood.to_csv(\"../Datasets/FastFoodRestaurants.csv\")\n",
    "\n",
    "# Merging fastfood data with income and health data\n",
    "temptemp = FastFood.groupby([\"State\", \"County\"]).count().reset_index()[['State','County','address']]\n",
    "tempo = temptemp.rename(columns={'address':'nr of FFchains'})\n",
    "data_df = pd.merge(new_df, tempo,  how='left', left_on=['State','County'], right_on =['State','County'])\n",
    "data_df['nr of FFchains'] = data_df['nr of FFchains'].fillna(0)\n",
    "\n",
    "# Removing ' County' from the county names in political data\n",
    "poldata[\"county_name\"] = poldata.county_name.str.replace(' County', '')\n",
    "\n",
    "# Merging the political data with the other data\n",
    "merged=pd.merge(data_df.copy(), poldata.copy(),  how='left', left_on=['State','County'], right_on = ['state_name','county_name'])\n",
    "\n",
    "# Snipping the columns to a more clean dataset\n",
    "data = merged[[\"State\",\"premature_deathYears_of_Potential_Life_Lost_Rate\",'adult_obesity_% Adults with Obesity',\n",
    "                \"adult_smoking_% Smokers\", \"excessive_drinking_% Excessive Drinking\", \"food_environment_index_Food Environment Index\",\n",
    "                \"uninsured_% Uninsured\", \"unemployed_% Unemployed\", 'nr of FFchains',\n",
    "                \"poor_physical_health_days_Average Number of Physically Unhealthy Days\",\n",
    "                \"poor_mental_health_days_Average Number of Mentally Unhealthy Days\",\"per_gop\"]]\n",
    "\n",
    "# Dropping NaNs\n",
    "data = data.dropna()\n",
    "\n",
    "# Creating a response value to predict a ML model\n",
    "data['is_obese'] = data['adult_obesity_% Adults with Obesity']>=33\n",
    "data = data.drop(['adult_obesity_% Adults with Obesity'],axis=1)\n",
    "\n",
    "# Cleansing columns, making unemployed percentage an integer and physical and mental unhealthy days floats\n",
    "data[\"unemployed_% Unemployed\"] = data[\"unemployed_% Unemployed\"].str.replace(\",\",\".\").astype(float).astype(int)\n",
    "data[\"poor_physical_health_days_Average Number of Physically Unhealthy Days\"] = data[\"poor_physical_health_days_Average Number of Physically Unhealthy Days\"].str.replace(\",\",\".\").astype(float)\n",
    "data[\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\"] = data[\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\"].str.replace(\",\",\".\").astype(float)\n",
    "\n",
    "# Storing the merged, cleansed dataset as a csv file\n",
    "data.to_csv(\"Datasets/Mixed_data.csv\")\n",
    "merged.to_csv(\"Datasets/All_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis.\n",
    "* Describe your data analysis and explain what you've learned about the dataset. *If relevant, talk about your machine-learning.\n",
    "\n",
    "### Statewise plot of health daata\n",
    "As we have alot of states we will start by plotting the contributers to loss of death prematurely, how many years of life that is lost based on premature death. \n",
    "The number of Fastfood restaurants per state and the percentage of obesity per state. \n",
    "\n",
    "We do this as our goal is to explore public health and well being, based on fastfood restaurants, and obesity is one major factor contributing to the loss of life prematurely. \n",
    "Likewise fastfood can lead to obesity. \n",
    "\n",
    "A simlpe bar plot can give us indications of how different states are ranked; likewise each state has a specific color so it is easier to navigate throughout the four plots as they can seem quite overwhelming\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with States abbrevs' \n",
    "FastFood['State'] = FastFood['province'].map(abbrev_to_us_state)\n",
    "States = health.copy()\n",
    "\n",
    "# Zero padding the FIPS code, as pandas don't accept leading 0's. \n",
    "States = States[States['FIPS'].astype(str).str.endswith('000')]\n",
    "\n",
    "#Define subplots to be a 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, sharex=False, figsize=(25,20))\n",
    "sns.set(rc={'figure.figsize':(12,9)})\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "s = States['State'].unique()\n",
    "\n",
    "palette = sns.color_palette(cc.glasbey, 51)\n",
    "\n",
    "hue_dic = dict(zip(s, palette))\n",
    "# Bar plot of prepamature deaths per state\n",
    "sns.barplot(ax=axes[0,0], data=States.sort_values(\"premature_deathDeaths\",ascending=False), y='State', x='premature_deathDeaths', palette=hue_dic).set(title='Total amount of Premature Deaths per state',  xlabel='Total number of premature deaths')\n",
    "# Years of potential life lost per state\n",
    "sns.barplot(ax=axes[0,1], data=States.sort_values(\"premature_deathYears_of_Potential_Life_Lost_Rate\",ascending=False), y='State', x='premature_deathYears_of_Potential_Life_Lost_Rate', palette=hue_dic).set(title='Amount of Potential years of life lost per state',  xlabel='Potential Life lost rate')\n",
    "# Number of fastfood restaurants per state\n",
    "sns.barplot(ax=axes[1,0], data=FastFood.groupby(['State']).count().reset_index().sort_values(\"address\",ascending=False), y='State', x='address', palette=hue_dic).set(title='Amount of Fastfood Resturants per state', xlabel='Total number of Fasfood Resturants')\n",
    "# Percentage of obesity per state \n",
    "sns.barplot(ax=axes[1,1], data=States.sort_values('adult_obesity_% Adults with Obesity',ascending=False), y='State', x='adult_obesity_% Adults with Obesity', palette=hue_dic).set(title='Percentage of Obese people per state', xlabel='Percentage of adult people with obesity')\n",
    "\n",
    "# Save subplot for website hosting as png file\n",
    "#fig.savefig(\"../Visulisations/4x4plot.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastfood restaurants\n",
    "\n",
    "We plot the *\"popularity\"* of the fastfood restaurants, to find our top 10. This will be our focus point. \n",
    "We did already see the plot with many colors, and to ease it up a bit for future use we decided to color each of the top 10 restaurants in the own destinctive color.\n",
    "As the top 10 accounts for the \"major\" contribution of restaurants across the US.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating index to the corresponding type of fastfood restaurant\n",
    "colorindex=(FastFood[\"Chain\"]==\"mcdonalds\")+(FastFood[\"Chain\"]==\"burger king\")*2+(FastFood[\"Chain\"]==\"taco bell\")*3+(FastFood[\"Chain\"]==\"wendys\")*4+(FastFood[\"Chain\"]==\"arbys\")*5+(FastFood[\"Chain\"]==\"kfc\")*6+(FastFood[\"Chain\"]==\"subway\")*7+(FastFood[\"Chain\"]==\"sonic drive in\")*8+(FastFood[\"Chain\"]==\"dominos pizza\")*9\n",
    "\n",
    "# Colors for the fastfood restaurants \n",
    "# Yellow Mcdonalds, Orange for Burger King, Purple for Taco bell, Red for Wendys, Green for Arbys, KFC Beige, Dark green Subway, Blue for Sonic, Dark blue for Domino's, Dark grey for Other\n",
    "restaurants=['Other',\"Mcdonald's\", 'Burger King', 'Taco Bell', \"Wendy's\", \"Arby's\", 'KFC', 'Subway', 'Sonic drive-in', \"Domino's Pizza\"]\n",
    "colors=[\"#4A4A4A\",\"#FFFF00\", \"#FF6103\", \"#8A2BE2\",\"#FF3030\", \"#00C957\", \"#FFE4C4\", \"#228B22\", \"#1E90FF\", \"#104E8B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the dark color for each of the other chains that are in the 'Other' group\n",
    "for i in range(len(FastFood[\"Chain\"].unique())-9):\n",
    "    colors.append(\"#4A4A4A\")\n",
    "\n",
    "# Bar plot\n",
    "FastFood[\"Chain\"] = FastFood[\"name\"].str.lower().str.replace(r\"[\\\"\\',]\", '').str.replace(r\"\\-\",\" \")\n",
    "sns.barplot(data=FastFood.groupby(['Chain']).count().reset_index().sort_values('address',ascending=False)[0:50], y='Chain', x='address', palette=colors).set(title='Number of Resturants per the 50 most popular chains', xlabel='Number of Resturants')\n",
    "\n",
    "# Saving the figure to plot in the website\n",
    "#plt.savefig('../docs/exploring_the_data/stat2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the knowledge of the major contributers for restaurants across the US, it was time to create a location of map, to see how they might accumulate throughout the country. We expect that a vast majority of the restaurants will be around the major cities. \n",
    "For this a Folium map, with the colors decided previously for the 10 major restaurants and `other` fits nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates for USA\n",
    "lat, lon = 37.77919, -100.41914 \n",
    "\n",
    "# Lists of coordinates for fastfood restaurants\n",
    "lat_list = list(FastFood.latitude)\n",
    "lon_list = list(FastFood.longitude)\n",
    "\n",
    "# Creating map and point layers\n",
    "map_FF = folium.Map([lat, lon], tiles = \"Stamen Toner\", zoom_start=4)\n",
    "point_layer = [folium.FeatureGroup(name=restaurants[0]), folium.FeatureGroup(name=restaurants[1]), folium.FeatureGroup(name=restaurants[2]), folium.FeatureGroup(name=restaurants[3]), folium.FeatureGroup(name=restaurants[4]), folium.FeatureGroup(name=restaurants[5]), folium.FeatureGroup(name=restaurants[6]), folium.FeatureGroup(name=restaurants[7]), folium.FeatureGroup(name=restaurants[8]), folium.FeatureGroup(name=restaurants[9])]\n",
    "\n",
    "# Creating the points to the map in the respective layer\n",
    "for i in range(len(FastFood)):\n",
    "    point_layer[colorindex[i]].add_child(folium.Circle(location=[lat_list[i], lon_list[i]], radius=50,\n",
    "    color=colors[colorindex[i]],\n",
    "    fill=True,\n",
    "    # Adding HTML styling for more detail to the pop up.\n",
    "    popup=folium.Popup(f\"\"\"<b>Name: </b>  {restaurants[colorindex[i]]} <br>\n",
    "                               <b>Address: </b>  {FastFood[\"address\"][i]} <br>\n",
    "                               <b>State: </b>  {FastFood[\"State\"][i]} <br>\n",
    "                               <b>County: </b> {FastFood[\"County\"][i]}\n",
    "                            \"\"\", max_width=len(f\"name= {restaurants[colorindex[i]]}\")*20),\n",
    "    fill_color=\"#3186cc\", tooltip = restaurants[colorindex[i]])).add_to(map_FF)\n",
    "    \n",
    "# Adding the point layers to the map\n",
    "map_FF.add_child(point_layer[0])\n",
    "map_FF.add_child(point_layer[1])\n",
    "map_FF.add_child(point_layer[2])\n",
    "map_FF.add_child(point_layer[3])\n",
    "map_FF.add_child(point_layer[4])\n",
    "map_FF.add_child(point_layer[5])\n",
    "map_FF.add_child(folium.LayerControl())     \n",
    "\n",
    "# Showing the map\n",
    "map_FF.save(\"Fastfood_locations.html\") # Save map as HTML for hosting on other GitHub Repo\n",
    "map_FF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our Surprise a vast majority of the restaurants were located on the east side of the country. We expected a more even distribution. \n",
    "However after realising that 36% of the United States' population is housed on the east side **[[14]](https://worldpopulationreview.com/state-rankings/east-coast-states)** it made alot more sense that the accumulated restaurants would be placed there. \n",
    "\n",
    "\n",
    "Looking at the West Coast distribution we see a pattern of accumulated restaurants around the big cities like Washington, Los Angeles and in Nevada Las Vegas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Food Enviroment Index' \n",
    "With knowledge of how the states performed health wise and county wise, and knowing of where the FastFood restaurants are located, we explored the `Food Envirometn Index Scoring`. \n",
    "This tells something about; the salery in that area vs possibilty to buy healthy food, and grocery store distribution. \n",
    "\n",
    "The goal was to find some sort of correlation between a bad food enviroment index and perhaps the ease of access to fastfood as a quick and cheap alternative; compared driving further to a grocery store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a barplot in decreasing order of the state and the corresponding food enviroment index score\n",
    "sns.barplot(data=States.sort_values(\"food_environment_index_Food Environment Index\",ascending=False), y='State', x='food_environment_index_Food Environment Index', palette=hue_dic).set(title='Food Enviornment Index per state',  xlabel='Food Enviornment Index')\n",
    "# Save the plot for GitHub Repository hosting\n",
    "# plt.savefig('../Visulisations/FoodEnv.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Jersy came out on top, and California which previously in account of percentage of obese people were 2nd lowest and had the most fastfood chains was also in the top range of the scoring. \n",
    "\n",
    "To explore county wise; we created a *Choropleth - map*, this would make it easier for us to explore the counties and areas by having a heatmap overview, but also for the reader to themselves explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring County wise Food Enviroment Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the locations for each fastfood restaurants were available, we tried to get an idea of the distribution of the food enviroment index county wise. \n",
    "This could perhabs reveal a pattern between the restaurant locations and the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plotly Chropleth map\n",
    "fig = px.choropleth_mapbox(health, geojson=counties, locations='FIPS', color=\"food_environment_index_Food Environment Index\",\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, 10),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3, center = {\"lat\": 37.77919, \"lon\": -100.41914},\n",
    "                           opacity=0.5,\n",
    "                           labels={'food_environment_index_Food Environment Index':'Food Enviornement Index'},\n",
    "                           hover_data=[\"State\", \"County\"] # Add additional info to mouse-overs (Hovers) \n",
    "                          )\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "\n",
    "# Save the plot as HTML, for hosting on the GitHub Repository website. \n",
    "#fig.write_html(\"foodindx.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to extract from the plot is the fact that the FEI (Food Enviroment Index) rankings top states are located on the east side of the country. Likewise they are most likely northern states. \n",
    "\n",
    "As we have the option within our data and our goal is to describe obesity and food quality in the US, we decided to create a similar plot; just describing the adult obesity percentage in each county. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### County Obesity percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plotly Choropleth map with the Obesity percentage countyswise. \n",
    "fig = px.choropleth_mapbox(health.fillna(0), geojson=counties, locations='FIPS', color='adult_obesity_% Adults with Obesity',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, max(health['adult_obesity_% Adults with Obesity'])),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3.3, center = {\"lat\": 37.77919, \"lon\": -100.41914},\n",
    "                           opacity=0.5,\n",
    "                           labels={'adult_obesity_% Adults with Obesity':'Adult obesity percentage'},\n",
    "                           hover_data=[\"State\", \"County\"] # Add additional info to mouse-overs (Hovers) \n",
    "                          )\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "\n",
    "# Save as HTML for GitHub  Repository hosting\n",
    "#fig.write_html(\"obesitypercentage.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, with the `Adult obesity percentage` and `Food Enviroment Index` we see a correlation especially on the west coast of counties that have a low percentage of obesed citizens, usually also have a fairly high FEI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further explore the data, we also plot the polical orientation in the year of 2020 to see if there's any correlation between political belief and obesity (health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared dataframe from the data handling in the beginning.\n",
    "tmp_data = pd.read_csv(\"Datasets/All_data.csv\", delimiter=\",\", index_col=0)\n",
    "# Convert fips to have loading 0's\n",
    "tmp_data['FIPS']=tmp_data['FIPS'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "data2 = pd.read_csv(\"Datasets/Mixed_data.csv\", delimiter=\",\", index_col=0)\n",
    "\n",
    "# import State and County to All_data -- Again #Hotfixing\n",
    "tmp_data[\"State\"] = health[\"State\"]\n",
    "tmp_data[\"County\"] = health[\"County\"]\n",
    "tmp_data[\"is_obese\"] = data2[\"is_obese\"]\n",
    "\n",
    "# Rename columns as a hotfix\n",
    "tmp_data = tmp_data.rename(columns={\"per_dem\": \"Democrate percentage\", \"per_gop\": \"Republican percentage\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plotly Choropleth map with the Obesity percentage countyswise. \n",
    "fig = px.choropleth_mapbox(tmp_data, geojson=counties, locations='FIPS', color='Republican percentage',\n",
    "                           color_continuous_scale=\"Bluered\",\n",
    "                           range_color=(0, 1),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3.3, center = {\"lat\": 37.77919, \"lon\": -100.41914},\n",
    "                           opacity=0.5,\n",
    "                           labels={'Republican percentage':'Republican percentage'},\n",
    "                           hover_data= [\"State\", \"County\", \"Democrate percentage\", \"is_obese\"]# Add additional info to mouse-overs (Hovers) \n",
    "                          )\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(\"republicancentage.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning modelling\n",
    "\n",
    "\n",
    "Based on our information on the data, we sought out to **predict obesity in states**. \n",
    "Rather than focusing only on the attributes we described above, we wanted to create a model based on the entire feature set of all data we had. The thought of this was lastly that we could do a Variance explained by feature exploration to check which attributes contributed the greatest as to whether or not a county in a state was obese or not. To give a reasonable goal, we had an expectation of predicting whether or not 1/3 of the adults in a state were obese or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by preparing the mixed data, that consists of data from multiple datasets. - In the data preperation we saved a file of mixed data sets. This is loaded and used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the mixed data sets\n",
    "# Loading the data\n",
    "data = pd.read_csv(\"Datasets/Mixed_data.csv\", delimiter=\",\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Correlation plot to identify possible correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temp dataframe with nicer labels for correaltion heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettylabels=['Years of potential life lost rate', 'Adult smokers %', 'Excessive drinkers %', 'Food Environment Index', 'Uninsured %', 'Unemployed %', 'No. fastfood restaurants','No physically unhealthy days', 'No mentally unhealthy days', 'Percentage Republican Voters']\n",
    "data.head()\n",
    "tmp_data = data.rename(columns = {\"State\": \"State\", \"premature_deathYears_of_Potential_Life_Lost_Rate\": prettylabels[0], \"adult_smoking_% Smokers\": prettylabels[1], \"excessive_drinking_% Excessive Drinking\": prettylabels[2], \"food_environment_index_Food Environment Index\": prettylabels[3], \"uninsured_% Uninsured\": prettylabels[4], \"unemployed_% Unemployed\": prettylabels[5], \"nr of FFchains\": prettylabels[6], \"poor_physical_health_days_Average Number of Physically Unhealthy Days\": prettylabels[7],\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\": prettylabels[8], \"per_gop\": prettylabels[9], \"is_obese\": \"Is_obese\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(tmp_data.corr(), cmap=\"YlGnBu\", annot=True )\n",
    "# Save for later\n",
    "plt.savefig('corrHeat.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap we learn whether or not some features are correlated or not, \n",
    "Interrestingly political beliefs seem to have some correlation with the general health of that specific County. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we need test and training data, and the test data cannot be included in the training data we have to sort out some states; so when predicting, the data is unknown for the model! \n",
    "\n",
    "For this we chose the tree states; Oregon (West-coast), Oklahoma (Midderteranian), Pennsylvania(East-coast). We chose three states in different locations as we saw previously that the distribution of restaurants did matter for how the FEI was placed; but not the obesity percentage. By having 3 states in the 3 extremes, we could explore the relation of health between east, middle and west side of the country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we start by setting up a `DecisionTreeClassifier`, from here we can then improve the model and see if having *multiple* `DecisionTreeClassifiers`i.e a `RandomForrestClassifier` will have an effect on the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further imports relevant to the Machine Learning part\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set random seed for replication of results. \n",
    "random_state = 42\n",
    "\n",
    "# Define model classifier\n",
    "clf = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "# Split data into - Predict values (y) and model paramters (X)\n",
    "y = data['is_obese']\n",
    "X = data.drop(['is_obese'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target states and split into train and test data! \n",
    "target_states = ['Oregon', 'Oklahoma', 'Pennsylvania']\n",
    "X_train = data[~data['State'].isin(target_states)]\n",
    "X_test = data[data['State'].isin(target_states)]\n",
    "y_train = X_train[\"is_obese\"]\n",
    "y_test = X_test[\"is_obese\"]\n",
    "X_train = X_train.drop(['is_obese', 'State'],axis=1)\n",
    "X_test = X_test.drop(['is_obese', 'State'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicng obesity based on features and data.     \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Extract accuracy measures \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "#return tn, fp, fn, tp\n",
    "# plot a confusio matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Customising the confusion matrix with a heatmap and scores. \n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "heatmap = sns.heatmap(cf_matrix, annot=labels, fmt='')\n",
    "fig = heatmap.get_figure()\n",
    "\n",
    "fig.show()\n",
    "fig.savefig(\"confusionMatrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "F1 = 2*precision*recall/(precision + recall)\n",
    "\n",
    "print(\"Accuracy of the model:\", accuracy)\n",
    "print(\"Precision of the model:\", precision)\n",
    "print(\"recall of the model:\", recall)\n",
    "print(\"F1-score of the model:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scores above reveal, there's definetely room for improvement. This can by done by trying with a `RandomForrestClassifier`- This will create a *forrest* of `DecisionTreeClassifiers`and have a majority vote based classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a `RandomForrestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the RandomForrestClassifier model\n",
    "\n",
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# Training the model using the splits made above\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting with the trained model \n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Getting the scores as previously, with the confusion matrix.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "#return tn, fp, fn, tp\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "heatmap = sns.heatmap(cf_matrix, annot=labels, fmt='')\n",
    "fig = heatmap.get_figure()\n",
    "fig.savefig(\"confusionMatrix_RF.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "F1 = 2*precision*recall/(precision + recall)\n",
    "\n",
    "print(\"Accuracy of the model:\", accuracy)\n",
    "print(\"Precision of the model:\", precision)\n",
    "print(\"recall of the model:\", recall)\n",
    "print(\"F1-score of the model:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definetely an improvement over the `DecisionTreeClassifier`, however the `RandomForrestClassifier`has alot of *hyperparameters* which can be tuned to further improve the models prediction rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by doing a `RandomSearchCV`as this is randomly searching through parameters while performing 5-fold crossvalidation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# number of trees in random forrest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num=10)]\n",
    "#Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 20, 30, 40, 50]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 6, 8, 10, 15, 20]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE FOLLOWING SECTION HAS COMMENTED CODE TO AVOID RETRAINING A MODEL - UNCOMMENT TO RE-FIND THE HYPERPARAMETERS - OR CONTENIUE AND USE THE PARAMETERS ALREADY FOUND** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following section to run the `RandomSearchCV`- uncomment due to timeconstraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create base model as previously\n",
    "#rf = RandomForestClassifier(random_state=random_state)\n",
    "## Use the classifier in the randomizedd search cv\n",
    "#rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv = 5, verbose=1, random_state=random_state, n_jobs=-1)\n",
    "#\n",
    "#rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more info of the status, increase the verbose level.\n",
    "With the randomsearhchperformed, we can print the parameters and further narrow down the search for the most optimal model by using `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing best params\n",
    "#rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, we do a specific search through a grid that is more \"identical to the parameters mentioned above\" - Creating a narrow grid of oppertunities in order to hopefully find the same or an even better set of *hyperparameters* for the model\n",
    "\n",
    "Uncomment the following section to perform the `GridSearchCV`- Or continue to retrieve and use the model already found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "## Based on the parameters we got (uncomment to retrieve the same)\n",
    "#param_grid = {\n",
    "#    'bootstrap': [True],\n",
    "#    'max_depth': [35,37,40,43,46],\n",
    "#    'max_features': [\"auto\"],\n",
    "#    'min_samples_leaf': [17,20,22,25],\n",
    "#    'min_samples_split': [25, 27, 30 ,33, 35],\n",
    "#    'n_estimators': [100, 150, 175, 200, 225, 250, 300]\n",
    "#}\n",
    "## Define Random grid variable for the classifier\n",
    "#rf_random_grid = RandomForestClassifier(random_state=random_state)\n",
    "#\n",
    "#grid_search = GridSearchCV(estimator=rf_random_grid, param_grid=param_grid, cv = 5, n_jobs=-1, verbose=1)\n",
    "#grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the model, whit the above printed *Hyperparameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RandomForestClassifier(bootstrap=True, max_depth=35, max_features=\"auto\", min_samples_leaf=20, min_samples_split= 25, n_estimators=150, random_state=random_state)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "final_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Defining a confusion matrix as previously\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "#return tn, fp, fn, tp\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "heatmap = sns.heatmap(cf_matrix, annot=labels, fmt='')\n",
    "fig = heatmap.get_figure()\n",
    "fig.show()\n",
    "fig.savefig(\"confusionMatrix_RF_tuned.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "F1 = 2*precision*recall/(precision + recall)\n",
    "\n",
    "print(\"Accuracy of the model:\", accuracy)\n",
    "print(\"Precision of the model:\", precision)\n",
    "print(\"recall of the model:\", recall)\n",
    "print(\"F1-score of the model:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the hyperparameter tuning the model performance didn't increase that much. However the general recall which is based on the number of `True positive` increased. \n",
    "This is quite usefull in a model as this, as we would rather assign one county to much as being obesed compared to not doing so; as it won't affect *human life* or might only lead to additional ressourcing being put into combating obesity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how the model predicts, we plot the 10 most significant features along with their standard deviations in the following plot. As to the project goal being predicting obesity and learning the factors behind, with a focus on Fastfood restaurants it is interessting to see what the model believes is the most important features from the data we had available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettylabels=['Years of potential life lost rate', 'Adult smokers %', 'Excessive drinkers %', 'Food Environment Index', 'Uninsured %', 'Unemployed %', 'No. fastfood restaurants','No physically unhealthy days', 'No mentally unhealthy days', 'Percentage Republican Voters']\n",
    "importances = pd.DataFrame({'Features':X_test.columns,\n",
    "                            'Importance':final_model.feature_importances_, 'Labels':prettylabels})\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\n",
    "stds=np.std([tree.feature_importances_ for tree in final_model.estimators_], axis=0)\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.barplot(data=importances.sort_values(\"Importance\",ascending=False), y='Features', x='Importance')\n",
    "# list(importances.sort_values(\"Importance\",ascending=False)[\"Features\"])\n",
    "ax.set_yticks(list(reversed(range(len(importances)))),list(importances.sort_values(\"Importance\",ascending=True)[\"Labels\"]))\n",
    "#stds=np.std([tree.feature_importances_ for tree in bestModel.estimators_], axis=0)\n",
    "\n",
    "ax.errorbar(data=importances.sort_values(\"Importance\",ascending=False), y='Features', x='Importance', yerr=None, xerr=stds,fmt='o',ecolor='black')\n",
    "\n",
    "plt.savefig(\"Barplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obove, Somking, political belief, mental unhealthy days, Excessive drinking are among 5 most important features. Whilst the number of Fastfood restaurants are still significant, but not as much as the other features! However \n",
    "keep inmind the affect that smoking has for many things, the Food enviroment index and No. fastfood restaurants hast quite a significant impact on whether a state is obesed or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evalutate the model, we plotted the results of our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "comp={'is_obese': y_test, 'predictions': y_pred, 'correct': y_test == y_pred, 'FIPS':merged['FIPS'][X_test.index], 'State':merged['State'][X_test.index]} \n",
    "comparison=pd.DataFrame(comp)\n",
    "#comparison.reset_index()\n",
    "#data['FIPS']=data['FIPS'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# fig = px.choropleth_mapbox(data, geojson=counties, locations='FIPS', color=\"is_obese\",\n",
    "#                            color_continuous_scale=\"Viridis\",\n",
    "#                            range_color=(0, 10),\n",
    "#                            mapbox_style=\"carto-positron\",\n",
    "#                            zoom=3, center = {\"lat\": 37.77919, \"lon\": -100.41914},\n",
    "#                            opacity=0.5,\n",
    "#                            labels={'food_environment_index_Food Environment Index':'Food Enviornement Index'}\n",
    "#                           )\n",
    "\n",
    "# Plotting the predicted states\n",
    "fig = px.choropleth_mapbox(comparison, geojson=counties, locations='FIPS', color=\"correct\",\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, 10),\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3, center = {\"lat\": 37.77919, \"lon\": -100.41914},\n",
    "                           opacity=0.5,\n",
    "                           labels={'predictions'}\n",
    "                          )\n",
    "\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(\"predctive_overview.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "def figures_to_html(figs, filename=\"dashboard.html\"):\n",
    "    with open(filename, 'w') as dashboard:\n",
    "        dashboard.write(\"<html><head></head><body>\" + \"\\n\")\n",
    "        for fig in figs:\n",
    "            inner_html = fig.to_html().split('<body>')[1].split('</body>')[0]\n",
    "            dashboard.write(inner_html)\n",
    "        dashboard.write(\"</body></html>\" + \"\\n\")\n",
    "\n",
    "#creating a small comparison dataframe to easily plot the data\n",
    "comp={'is_obese': y_test, 'predictions': y_pred, 'correct': y_test == y_pred, 'FIPS':merged['FIPS'][X_test.index], 'State':merged['State'][X_test.index]} \n",
    "comparison=pd.DataFrame(comp)\n",
    "\n",
    "#Defining States and Fips for each state and a colorscale for True/False\n",
    "Oregon_values = comparison[comparison['State'] == 'Oregon']['correct'].tolist()\n",
    "Oregon_fips = comparison[comparison['State'] == 'Oregon']['FIPS'].tolist()\n",
    "Oklahoma_values = comparison[comparison['State'] == 'Oklahoma']['correct'].tolist()\n",
    "Oklahoma_fips = comparison[comparison['State'] == 'Oklahoma']['FIPS'].tolist()\n",
    "Penn_values = comparison[comparison['State'] == 'Pennsylvania']['correct'].tolist()\n",
    "Penn_fips = comparison[comparison['State'] == 'Pennsylvania']['FIPS'].tolist()\n",
    "colorscale = [\"#E60000\",\"#0000CD\"]\n",
    "\n",
    "#Creating figure for Oklahoma\n",
    "fig1 = ff.create_choropleth(\n",
    "    fips=Oklahoma_fips, values=Oklahoma_values, scope=['Oklahoma'],\n",
    "    colorscale=colorscale, round_legend_values=True,\n",
    "    simplify_county=0, simplify_state=0,\n",
    "    county_outline={'color': 'rgb(15, 15, 55)', 'width': 0.5},\n",
    "    state_outline={'width': 1},\n",
    "    legend_title='Correct Prediction',\n",
    "    title='Oklahoma'\n",
    ")\n",
    "\n",
    "#Creating figure for Oregon\n",
    "fig2 = ff.create_choropleth(\n",
    "    fips=Oregon_fips, values=Oregon_values, scope=['Oregon'],\n",
    "    colorscale=colorscale, round_legend_values=True,\n",
    "    simplify_county=0, simplify_state=0,\n",
    "    county_outline={'color': 'rgb(15, 15, 55)', 'width': 0.5},\n",
    "    state_outline={'width': 1},\n",
    "    legend_title='Correct Prediction',\n",
    "    title='Oregon'\n",
    ")\n",
    "\n",
    "#Creating figure for Pennsylvania\n",
    "fig3 = ff.create_choropleth(\n",
    "    fips=Penn_fips, values=Penn_values, scope=['Pennsylvania'],\n",
    "    colorscale=colorscale, round_legend_values=True,\n",
    "    simplify_county=0, simplify_state=0,\n",
    "    county_outline={'color': 'rgb(15, 15, 55)', 'width': 0.5},\n",
    "    state_outline={'width': 1},\n",
    "    legend_title='Correct Prediction',\n",
    "    title='Pennsylvania'\n",
    ")\n",
    "\n",
    "#Due to some flaw in the package, the hoverdata gets erased. A hotfix for it was found and implemented below.\n",
    "#The hotfix more or less duplicates the intended hovertext and adds it back on\n",
    "#Hotfix for Oregon map\n",
    "hover_ix, hover = [(ix, t) for ix, t in enumerate(fig2['data']) if t.text][0]\n",
    "df_sample_r = comparison[comparison['State'] == 'Oregon']\n",
    "# mismatching lengths indicates bug\n",
    "if len(hover['text']) != len(df_sample_r):\n",
    "\n",
    "    ht = pd.Series(hover['text'])\n",
    "\n",
    "    no_dupe_ix = ht.index[~ht.duplicated()]\n",
    "\n",
    "    hover_x_deduped = np.array(hover['x'])[no_dupe_ix]\n",
    "    hover_y_deduped = np.array(hover['y'])[no_dupe_ix]\n",
    "\n",
    "    new_hover_x = [x if type(x) == float else x[0] for x in hover_x_deduped]\n",
    "    new_hover_y = [y if type(y) == float else y[0] for y in hover_y_deduped]\n",
    "\n",
    "    fig2['data'][hover_ix]['text'] = ht.drop_duplicates()\n",
    "    fig2['data'][hover_ix]['x'] = new_hover_x\n",
    "    fig2['data'][hover_ix]['y'] = new_hover_y\n",
    "\n",
    "#Same hotfix for Pennsylvania\n",
    "hover_ix, hover = [(ix, t) for ix, t in enumerate(fig3['data']) if t.text][0]\n",
    "df_sample_r = comparison[comparison['State'] == 'Pennsylvania']\n",
    "# mismatching lengths indicates bug\n",
    "if len(hover['text']) != len(df_sample_r):\n",
    "\n",
    "    ht = pd.Series(hover['text'])\n",
    "\n",
    "    no_dupe_ix = ht.index[~ht.duplicated()]\n",
    "\n",
    "    hover_x_deduped = np.array(hover['x'])[no_dupe_ix]\n",
    "    hover_y_deduped = np.array(hover['y'])[no_dupe_ix]\n",
    "\n",
    "    new_hover_x = [x if type(x) == float else x[0] for x in hover_x_deduped]\n",
    "    new_hover_y = [y if type(y) == float else y[0] for y in hover_y_deduped]\n",
    "\n",
    "    fig3['data'][hover_ix]['text'] = ht.drop_duplicates()\n",
    "    fig3['data'][hover_ix]['x'] = new_hover_x\n",
    "    fig3['data'][hover_ix]['y'] = new_hover_y\n",
    "\n",
    "figures_to_html([fig1,fig2,fig3], filename = 'Predict_states.html')\n",
    "\n",
    "#figures_to_html([fig1], filename = 'Oregon.html')\n",
    "#figures_to_html([fig2], filename = 'Oklahoma.html')\n",
    "#figures_to_html([fig3], filename = 'Pennsylvania.html')\n",
    "#with open('p_graph.html', 'a') as f:\n",
    " #   f.write(fig1.to_html(full_html=True, include_plotlyjs='cdn'))\n",
    "  #  f.write(fig2.to_html(full_html=True, include_plotlyjs='cdn'))\n",
    "   # f.write(fig3.to_html(full_html=True, include_plotlyjs='cdn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_ix, hover = [(ix, t) for ix, t in enumerate(fig2['data']) if t.text][0]\n",
    "df_sample_r = comparison[comparison['State'] == 'Oregon']\n",
    "# mismatching lengths indicates bug\n",
    "if len(hover['text']) != len(df_sample_r):\n",
    "\n",
    "    ht = pd.Series(hover['text'])\n",
    "\n",
    "    no_dupe_ix = ht.index[~ht.duplicated()]\n",
    "\n",
    "    hover_x_deduped = np.array(hover['x'])[no_dupe_ix]\n",
    "    hover_y_deduped = np.array(hover['y'])[no_dupe_ix]\n",
    "\n",
    "    new_hover_x = [x if type(x) == float else x[0] for x in hover_x_deduped]\n",
    "    new_hover_y = [y if type(y) == float else y[0] for y in hover_y_deduped]\n",
    "\n",
    "    fig2['data'][hover_ix]['text'] = ht.drop_duplicates()\n",
    "    fig2['data'][hover_ix]['x'] = new_hover_x\n",
    "    fig2['data'][hover_ix]['y'] = new_hover_y\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts on the model\n",
    "\n",
    "Overall the model predicts quite well. One wouldn't expect a model with a 100% accuracty so to get most of them right across 3 different regions in the US is a pretty decent model. Especially considering that the culture is pretty standardized by yet so different from west US to the east of US. \n",
    "\n",
    "To further improve the model, one could try other classification methods which are more advanced; this could be neural networks or performing for an example PCA, for reducing the number of features to the most relevant ones and gaining an overall simpler model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre.\n",
    "The genre we chose to use is **Magazine Style** with some **Slide Show** features. Our focus from the beginning was the story and how the visualization should fit together with the story, the style to tell the story came more naturally as we began to construct visualizations, analyze those and started to build a webpage. \n",
    "\n",
    "We chose this genre since our visualizations are overall quite detailed and the reader has the opportunity to spend time go in depth with exploring the visualizaztions. The corresponding text is also rather in depth, however using bold and emojis trying to make it more readable, such as skimming the respective webpages also should be able to give an impression on what is going on, with the opportunity to go in depth by reading and exploring the interactive plots. \n",
    "\n",
    "There is somewhat linearity with the story, however not completely bound, the sidebar makes it easy for the reader to jump around to the headlines that spark interest, and the pages has some introduction in the beginning such that it is not strictly nescessary to have read the other pages, however that would be the intuitive way to click around. Therefore it can be argued that it is mostly **author-driven** in regards to somewhat **linear ordering of scenes** and as there is (if the reader chooses to go in depth with the text) a lot of **messaging**. In regards to the webpage structure and (some) of the visualizations it is **reader-driven** as the reader has the opportunity to click around and explore the **interactive website** and **interactive map plots**. \n",
    "\n",
    "## Visual Narrative\n",
    "\n",
    "| **Visual structuring** \t| *** \t|\n",
    "|---:\t|---:\t|\n",
    "| Establishing Shot/Spalsh Screen \t| + \t|\n",
    "| Consistent Visual Platform  \t| + \t|\n",
    "| Progress Bar/Timebar  \t| - \t|\n",
    "| \"Checklist\" Progress Tracker \t| - \t|\n",
    "| **Highlight** \t| *** \t|\n",
    "| Close-Ups \t| + \t|\n",
    "| Feature Distinction  \t| - \t|\n",
    "| Character Direction  \t| - \t|\n",
    "| Motion \t| + \t|\n",
    "| Audio \t| - \t|\n",
    "| Zooming \t| + \t|\n",
    "| **Transition Guidance** \t| *** \t|\n",
    "| Familiar Objects  \t| + \t|\n",
    "| Viewing Angle  \t| - \t|\n",
    "| Viewer (Camera) Motion  \t| - \t|\n",
    "| Continuity Editing \t|- \t|\n",
    "| Object Continuity \t| - \t|\n",
    "| Animated Transitions \t| - \t|\n",
    "\n",
    "**Visual Structuring** \n",
    "\n",
    "\n",
    "We chose to have an establishing shot for our web page to have a more fun and inspiring introduction to reading about what we did in the project. We chose a consistent visual platform as the plots themselves differed from barplots, maps and correlation-/confusionmatrices, and therefore the background was more consistent as to create more consitency throughout the plot. It would have been nice with a progress bar, but we valued spending more time on the visualizations than on implementing such. \n",
    "\n",
    "**Highlight** \n",
    "\n",
    "\n",
    "The interactive maps gave the opportunity to zoom and investigate close on fastfood chains and the heat maps, furthermore the three chosen test-states were zoom in on in the model evaluation, this was done to get a better view of the county data. Motion was necessary to be able to use within the webpage, since there was large plots and text in each webpage, however the motion was limited to scrolling. \n",
    "\n",
    "**Transition Guidance** \n",
    "\n",
    "\n",
    "We added emojis to the text as familiar objects, also to make an easier visual connection to the text and make it more fun to read. Furthermore there was clicking buttons to guide the reader back and forth together with the siderbar. \n",
    "\n",
    "\n",
    "\n",
    "## Narrative Structure\n",
    "\n",
    "| **Ordering** \t\t| *** |\n",
    "|---:\t|---:\t|\n",
    "| Random Access \t| + \t|\n",
    "| User Directed Path  \t| - \t|\n",
    "| Linear  \t| + \t|\n",
    "| **Interactivity** \t| *** | \n",
    "| Hover Highlighting/Details \t| + \t|\n",
    "| Filterint/Selection/Search  \t| - \t|\n",
    "| Navigation Buttons  \t| + \t|\n",
    "| Very Limited Interactivity \t| - \t|\n",
    "| Explicit Intstruction \t| - \t|\n",
    "| Tacit Tutorial \t| - \t|\n",
    "| Stimulating default views \t| + \t|\n",
    "| **Messaging** \t| ***\t|\n",
    "| Captions/Headlines \t| + \t|\n",
    "| Annotations  \t| - \t|\n",
    "| Accompanying Article  \t| + \t|\n",
    "| Multi-messaging \t| + \t|\n",
    "| Comment Repition \t| - \t|\n",
    "| Introductory Text \t| + \t|\n",
    "| Summary \t| + \t|\n",
    "\n",
    "**Ordering**\n",
    "\n",
    "\n",
    "The storyline is overall linear guided with buttons back and forth in the bottom of the pages, with the possibility to access independent topics through the sidebar. \n",
    "\n",
    "**Interactivity** \n",
    "\n",
    "\n",
    "All the maps had interactivity with both hovertool for counties as well as for the fastfood restaurants. There was somewhat navigating buttons in the bottum of the pages and the ordering of the sidebar. Stimulating default views was used for the interactive maps as the startet in the view of USA, it could have been considered whether each start page of the different topics could be more stimulating, but it also shouldn't be overwhelming. \n",
    "\n",
    "**Messaging**  \n",
    "\n",
    "\n",
    "All the topics had headlines and subtopics which was foldable. The visualizations had accompanying article, which was necessary since the plots were not able to stand for themselves, they were not self explanatory. Multimessaging was used in a way, showing barplots and maps. The project had both introductory text as well as summary in the discussion section. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations.\n",
    "* We've chosen a combination of normal bar plots for a general understanding of rankings. \n",
    "As the vast majority of users will be familiar with these and have a knowledge of how to interpret them. \n",
    "Similarily they perform very well in presenting basic elements where we want to present rankings. We are able to show by which margain for an example the Restaurant chain McDonalds is dominant compared to others. \n",
    "\n",
    "* Likewise to obtain an understanding of model performance we've used Confusion Matrix' to present the number of True positives, false negatives etc. with a heatmap representation alike. \n",
    "* For last plot of plotting the feature importances in the model; we use a mix of a *bar* & *lollipop* plot to visualise the importance scores aswell as the standarddeviations. \n",
    "* For normal location and pinning we've used Folium maps, as they are easy to work with, likewise we've used \n",
    "plotly - Choropleth Maps for heat maps with geolocations. \n",
    "These types of plots where chosen as they allow for a \"default\" view, where we can tell our approach to the story. \n",
    "Meanwhile they add interactivity, which allows the reader to explore themselves how the data might present it selv and find new elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think critically about your creation\n",
    "\n",
    "* What went well \n",
    "\n",
    "The data we found was very useful, both health, fastfood and political data had almost all counties included. The data was useful in terms of visualizing it geographically. \n",
    "\n",
    "* What is still missing\n",
    "\n",
    "The income data was not very useful, and some time was spent on trying to interpolate with the missing counties, that however only worked in sort of pixel terms, which is why we ended up leaving it out of the machine learning model. Furthermore the county dataset was hard to visualize in other ways than in a geo map, since there were thousands of counties, the data could be aggregated as states, which was used in some barplots, but it was not the easiest way to visualize it, however it was easier to compare states in that way where the maps gave more of an overview. Furthermore we had some thoughts in regards to sex and race seggregated data, those thoughts were also discussed in the webpage in 'Discussion'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributions\n",
    "* Fastfood locations - Michella\n",
    "* Health data statewise - Gustav\n",
    "* Food Quality - Peetz\n",
    "* Political Orientation - Michella\n",
    "* Income - Gustav\n",
    "* Constructing and training model - Peetz\n",
    "* Feature evaluating - Michella\n",
    "* Prediction evaluation - Gustav\n",
    "* Webpage - Peetz"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
