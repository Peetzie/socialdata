{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook\n",
    "\n",
    "## Motivation.\n",
    "### The datasets\n",
    "Our data consists of 4 diffent datasets that describes the counties across the united states. Our final datasets contain 14 variables after cleaning and preprocessing and to name a few it contains adult obesity, mean income, poltical stance etc.\n",
    "\n",
    "### Why health, Fastfood chains & income data?\n",
    "One of the problems of some modern welfare states is a tendency of obesity. \n",
    "\n",
    "We have choosen health in the US because we would like to study how other social factors may have an impact on ones health. The Health data allows us to investigate many potential factors in determining obesity in the United States of America, these factors are: Income, exposure to fastfood restaurants, physical health, mental health, smoking habits, drinking habits, employment status and political orientation. \n",
    "\n",
    "The Fastfood chain data can also have an effect on the health. The trend seems that the Americans every year spend more money on take-away excluding 2020, however that year was also extraordinary in regards to lockdown caused by COVID-19. And the income data is just as relevant, as sources tells us that almost the same percentage of the American income is spend on take-away, where the percentage spend on homemade food is decreasing.\n",
    "\n",
    "#### The idea and goal of the project \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "* Write about your choices in data cleaning and preprocessing\n",
    "* Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis.\n",
    "\n",
    "### Choices in data cleaning and preprocessing\n",
    "\n",
    "#### County Health Rankings Dataset\n",
    "The Health Dataset consists of 3193 rows and 250 columns. A row corresponds to a county in the US and the first columns consists of a FIPS code, the name of the state the county is within and the name of the county. The rest of the columns describe different health factors of each county such as obesity, smokers, alcoholism, education etc.\n",
    "\n",
    "226 of the rows in the data has an x in a column named \"Unreliable\". The column is not further explained in the data description given in the [PDF of data description](https://www.countyhealthrankings.org/sites/default/files/media/document/DataDictionary_2020_2.pdf) but for the sake of the column name, these rows will be removed. \n",
    "\n",
    "Due to the way pandas can read a csv, the first zero of the FIPS code can be automatically omitted. This will not allow `plotly` to plot those states, which is why we need to apply a zero infront of the row if the number is less than 5 digits short using:\n",
    "\n",
    "`df['FIPS']=df'FIPS'].apply(lambda x: '{0:0>5}'.format(x))`\n",
    "\n",
    "Too easier combine the different datasets, a dictionary of the states and their abbreviation (`us_state_to_abbrev`) is needed to translate the states. This allows us to use `pandas.groupby` to combine the states and counties. It is important to groupby both State and County since some county names may repeat across different States.\n",
    "\n",
    "A few rows also had floats represented as a string, which had to be translated into floats to analyse.\n",
    "\n",
    "#### FastFood Chains across America\n",
    "The FastFood Dataset consists of 10000 rows and 14 columns. A row corresponds to a resturant in the US and the first columns consists of a address, the name of the Fastfood chain, the state etc. This dataset does not have a column for county, so we have to extract that information ourselves and create a new column to group this data together with the other datasets. Since the postalcode is a column in the dataset, we can use `pgeocode` to extract the county information for each resturant. \n",
    "\n",
    "```\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "county_names = []\n",
    "for i in range(len(FastFood)):\n",
    "    county_names.append(nomi.query_postal_code(FastFood[\"postalCode\"][i]).county_name)\n",
    "    \n",
    "FastFood[\"County\"] = county_names\n",
    "```\n",
    "\n",
    "The focus variable we are interested in from this data set is the a count of how many chains there is in each county as well as what fastfood chains we can see across the states.\n",
    "\n",
    "#### US Household Income Statistics & Political data\n",
    "\n",
    "The US Household Dataset consists of 32526 rows and 19 columns. Each row corresponds to some area code within a county. This and the Political data has the word *County* added to each string value in the `County` column, meaning we need to remove the last word of each element in this column. The focus variable we are interested in from the income data set is the `Mean` column which represents the mean income for households in that county and per_gop from the Political data.\n",
    "\n",
    "#### Data Cleaning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Unreliable</th>\n",
       "      <th>premature_deathDeaths</th>\n",
       "      <th>premature_deathYears_of_Potential_Life_Lost_Rate</th>\n",
       "      <th>premature_death_95% CILow</th>\n",
       "      <th>premature_death_95% CI - High</th>\n",
       "      <th>premature_death_Quartile</th>\n",
       "      <th>premature_death_YPLL Rate (AIAN)</th>\n",
       "      <th>...</th>\n",
       "      <th>state_name</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>county_name</th>\n",
       "      <th>votes_gop</th>\n",
       "      <th>votes_dem</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>diff</th>\n",
       "      <th>per_gop</th>\n",
       "      <th>per_dem</th>\n",
       "      <th>per_point_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01000</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82249.0</td>\n",
       "      <td>9820.0</td>\n",
       "      <td>9718.0</td>\n",
       "      <td>9922.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>787.0</td>\n",
       "      <td>7830.0</td>\n",
       "      <td>6998.0</td>\n",
       "      <td>8662.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>19838.0</td>\n",
       "      <td>7503.0</td>\n",
       "      <td>27770.0</td>\n",
       "      <td>12335.0</td>\n",
       "      <td>0.714368</td>\n",
       "      <td>0.270184</td>\n",
       "      <td>0.444184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3147.0</td>\n",
       "      <td>7680.0</td>\n",
       "      <td>7237.0</td>\n",
       "      <td>8124.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>83544.0</td>\n",
       "      <td>24578.0</td>\n",
       "      <td>109679.0</td>\n",
       "      <td>58966.0</td>\n",
       "      <td>0.761714</td>\n",
       "      <td>0.224090</td>\n",
       "      <td>0.537623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>515.0</td>\n",
       "      <td>11477.0</td>\n",
       "      <td>9908.0</td>\n",
       "      <td>13045.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>5622.0</td>\n",
       "      <td>4816.0</td>\n",
       "      <td>10518.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>0.534512</td>\n",
       "      <td>0.457882</td>\n",
       "      <td>0.076631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>476.0</td>\n",
       "      <td>12173.0</td>\n",
       "      <td>10506.0</td>\n",
       "      <td>13839.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>7525.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>9595.0</td>\n",
       "      <td>5539.0</td>\n",
       "      <td>0.784263</td>\n",
       "      <td>0.206983</td>\n",
       "      <td>0.577280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>56037</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Sweetwater</td>\n",
       "      <td>NaN</td>\n",
       "      <td>527.0</td>\n",
       "      <td>7775.0</td>\n",
       "      <td>6849.0</td>\n",
       "      <td>8701.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56037.0</td>\n",
       "      <td>Sweetwater</td>\n",
       "      <td>12229.0</td>\n",
       "      <td>3823.0</td>\n",
       "      <td>16603.0</td>\n",
       "      <td>8406.0</td>\n",
       "      <td>0.736554</td>\n",
       "      <td>0.230260</td>\n",
       "      <td>0.506294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>56039</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2980.0</td>\n",
       "      <td>2094.0</td>\n",
       "      <td>3866.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56039.0</td>\n",
       "      <td>Teton</td>\n",
       "      <td>4341.0</td>\n",
       "      <td>9848.0</td>\n",
       "      <td>14677.0</td>\n",
       "      <td>-5507.0</td>\n",
       "      <td>0.295769</td>\n",
       "      <td>0.670982</td>\n",
       "      <td>-0.375213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>56041</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Uinta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271.0</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>6637.0</td>\n",
       "      <td>9525.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56041.0</td>\n",
       "      <td>Uinta</td>\n",
       "      <td>7496.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>9402.0</td>\n",
       "      <td>5905.0</td>\n",
       "      <td>0.797277</td>\n",
       "      <td>0.169219</td>\n",
       "      <td>0.628058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>56043</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6541.0</td>\n",
       "      <td>4417.0</td>\n",
       "      <td>8665.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56043.0</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>3245.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>4012.0</td>\n",
       "      <td>2594.0</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.162263</td>\n",
       "      <td>0.646560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>56045</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3858.0</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>5842.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56045.0</td>\n",
       "      <td>Weston</td>\n",
       "      <td>3107.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>3542.0</td>\n",
       "      <td>2747.0</td>\n",
       "      <td>0.877188</td>\n",
       "      <td>0.101637</td>\n",
       "      <td>0.775551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3193 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FIPS    State      County Unreliable  premature_deathDeaths  \\\n",
       "0     01000  Alabama         NaN        NaN                82249.0   \n",
       "1     01001  Alabama     Autauga        NaN                  787.0   \n",
       "2     01003  Alabama     Baldwin        NaN                 3147.0   \n",
       "3     01005  Alabama     Barbour        NaN                  515.0   \n",
       "4     01007  Alabama        Bibb        NaN                  476.0   \n",
       "...     ...      ...         ...        ...                    ...   \n",
       "3188  56037  Wyoming  Sweetwater        NaN                  527.0   \n",
       "3189  56039  Wyoming       Teton        NaN                  109.0   \n",
       "3190  56041  Wyoming       Uinta        NaN                  271.0   \n",
       "3191  56043  Wyoming    Washakie        NaN                  104.0   \n",
       "3192  56045  Wyoming      Weston        NaN                   76.0   \n",
       "\n",
       "      premature_deathYears_of_Potential_Life_Lost_Rate  \\\n",
       "0                                               9820.0   \n",
       "1                                               7830.0   \n",
       "2                                               7680.0   \n",
       "3                                              11477.0   \n",
       "4                                              12173.0   \n",
       "...                                                ...   \n",
       "3188                                            7775.0   \n",
       "3189                                            2980.0   \n",
       "3190                                            8081.0   \n",
       "3191                                            6541.0   \n",
       "3192                                            3858.0   \n",
       "\n",
       "      premature_death_95% CILow  premature_death_95% CI - High  \\\n",
       "0                        9718.0                         9922.0   \n",
       "1                        6998.0                         8662.0   \n",
       "2                        7237.0                         8124.0   \n",
       "3                        9908.0                        13045.0   \n",
       "4                       10506.0                        13839.0   \n",
       "...                         ...                            ...   \n",
       "3188                     6849.0                         8701.0   \n",
       "3189                     2094.0                         3866.0   \n",
       "3190                     6637.0                         9525.0   \n",
       "3191                     4417.0                         8665.0   \n",
       "3192                     2418.0                         5842.0   \n",
       "\n",
       "      premature_death_Quartile  premature_death_YPLL Rate (AIAN)  ...  \\\n",
       "0                          NaN                            5145.0  ...   \n",
       "1                          1.0                               NaN  ...   \n",
       "2                          1.0                               NaN  ...   \n",
       "3                          3.0                               NaN  ...   \n",
       "4                          4.0                               NaN  ...   \n",
       "...                        ...                               ...  ...   \n",
       "3188                       3.0                               NaN  ...   \n",
       "3189                       1.0                               NaN  ...   \n",
       "3190                       4.0                               NaN  ...   \n",
       "3191                       2.0                               NaN  ...   \n",
       "3192                       1.0                               NaN  ...   \n",
       "\n",
       "      state_name  county_fips county_name  votes_gop  votes_dem  total_votes  \\\n",
       "0            NaN          NaN         NaN        NaN        NaN          NaN   \n",
       "1        Alabama       1001.0     Autauga    19838.0     7503.0      27770.0   \n",
       "2        Alabama       1003.0     Baldwin    83544.0    24578.0     109679.0   \n",
       "3        Alabama       1005.0     Barbour     5622.0     4816.0      10518.0   \n",
       "4        Alabama       1007.0        Bibb     7525.0     1986.0       9595.0   \n",
       "...          ...          ...         ...        ...        ...          ...   \n",
       "3188     Wyoming      56037.0  Sweetwater    12229.0     3823.0      16603.0   \n",
       "3189     Wyoming      56039.0       Teton     4341.0     9848.0      14677.0   \n",
       "3190     Wyoming      56041.0       Uinta     7496.0     1591.0       9402.0   \n",
       "3191     Wyoming      56043.0    Washakie     3245.0      651.0       4012.0   \n",
       "3192     Wyoming      56045.0      Weston     3107.0      360.0       3542.0   \n",
       "\n",
       "         diff   per_gop   per_dem  per_point_diff  \n",
       "0         NaN       NaN       NaN             NaN  \n",
       "1     12335.0  0.714368  0.270184        0.444184  \n",
       "2     58966.0  0.761714  0.224090        0.537623  \n",
       "3       806.0  0.534512  0.457882        0.076631  \n",
       "4      5539.0  0.784263  0.206983        0.577280  \n",
       "...       ...       ...       ...             ...  \n",
       "3188   8406.0  0.736554  0.230260        0.506294  \n",
       "3189  -5507.0  0.295769  0.670982       -0.375213  \n",
       "3190   5905.0  0.797277  0.169219        0.628058  \n",
       "3191   2594.0  0.808824  0.162263        0.646560  \n",
       "3192   2747.0  0.877188  0.101637        0.775551  \n",
       "\n",
       "[3193 rows x 273 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#loading datasets\n",
    "health = pd.read_csv(\"Datasets/rankmd.csv\", delimiter=\";\")\n",
    "FastFood = pd.read_csv(\"Datasets/FastFoodRestaurants.csv\")\n",
    "income = pd.read_csv(\"Datasets/kaggle_income.csv\", encoding=\"ISO 8859-1\")\n",
    "poldata = pd.read_csv(\"Datasets/2020_US_County_Level_Presidential_Results.csv\", delimiter=\",\")\n",
    "\n",
    "#dictionary of the states to abbreviation\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "    \n",
    "# Inverting the dictionary\n",
    "abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n",
    "\n",
    "# Creating a state dataset\n",
    "FastFood['State'] = FastFood['province'].map(abbrev_to_us_state)\n",
    "States = health.copy()\n",
    "States = States[States['FIPS'].astype(str).str.endswith('000')]\n",
    "\n",
    "# Converting FIPS to string\n",
    "health['FIPS']=health['FIPS'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "\n",
    "#Setting the food_enviornment index as float instead of string\n",
    "health[\"food_environment_index_Food Environment Index\"] = health[\"food_environment_index_Food Environment Index\"].str.replace(\",\",\".\").astype(float)\n",
    "\n",
    "# Removing ' County' from the county names in income\n",
    "income[\"County\"] = income.County.str.replace(' County', '')\n",
    "\n",
    "# Merging income and health data\n",
    "temp_df = income.groupby([\"State_Name\",\"County\"]).mean().reset_index()\n",
    "new_df = pd.merge(health.copy(), temp_df.copy(),  how='left', left_on=['State','County'], right_on = ['State_Name','County'])\n",
    "\n",
    "#ONLY NEEDS TO BE RAN ONCE AS THE COUNTIES ARE STORED IN THE CSV.\n",
    "\n",
    "#!{sys.executable} -m pip install pgeocode\n",
    "#import pgeocode\n",
    "\n",
    "#nomi = pgeocode.Nominatim('us')\n",
    "#county_names = []\n",
    "#for i in range(len(FastFood)):\n",
    "#    county_names.append(nomi.query_postal_code(FastFood[\"postalCode\"][i]).county_name)\n",
    "    \n",
    "#FastFood[\"County\"] = county_names\n",
    "#FastFood.to_csv(\"../Datasets/FastFoodRestaurants.csv\")\n",
    "\n",
    "# Merging fastfood data with income and health data\n",
    "temptemp = FastFood.groupby([\"State\", \"County\"]).count().reset_index()[['State','County','address']]\n",
    "tempo = temptemp.rename(columns={'address':'nr of FFchains'})\n",
    "data_df = pd.merge(new_df, tempo,  how='left', left_on=['State','County'], right_on =['State','County'])\n",
    "data_df['nr of FFchains'] = data_df['nr of FFchains'].fillna(0)\n",
    "\n",
    "# Removing ' County' from the county names\n",
    "poldata[\"county_name\"] = poldata.county_name.str.replace(' County', '')\n",
    "\n",
    "# Merging the political data with the other data\n",
    "merged=pd.merge(data_df.copy(), poldata.copy(),  how='left', left_on=['State','County'], right_on = ['state_name','county_name'])\n",
    "\n",
    "# Snipping the columns to a more clean dataset\n",
    "data = merged[[\"premature_deathYears_of_Potential_Life_Lost_Rate\",'adult_obesity_% Adults with Obesity',\n",
    "                \"adult_smoking_% Smokers\", \"excessive_drinking_% Excessive Drinking\", \"food_environment_index_Food Environment Index\",\n",
    "                \"uninsured_% Uninsured\", \"unemployed_% Unemployed\", 'nr of FFchains', 'Mean',\n",
    "                \"poor_physical_health_days_Average Number of Physically Unhealthy Days\",\n",
    "                \"poor_mental_health_days_Average Number of Mentally Unhealthy Days\",\"per_gop\"]]\n",
    "\n",
    "# Dropping NaNs\n",
    "data = data.dropna()\n",
    "\n",
    "# Creating a response value to predict a ML model\n",
    "data['is_obese'] = data['adult_obesity_% Adults with Obesity']>=33\n",
    "data = data.drop(['adult_obesity_% Adults with Obesity', 'Mean'],axis=1)\n",
    "\n",
    "# Cleansing columns, making unemployed percentage an integer and physical and mental unhealthy days floats\n",
    "data[\"unemployed_% Unemployed\"] = data[\"unemployed_% Unemployed\"].str.replace(\",\",\".\").astype(float).astype(int)\n",
    "data[\"poor_physical_health_days_Average Number of Physically Unhealthy Days\"] = data[\"poor_physical_health_days_Average Number of Physically Unhealthy Days\"].str.replace(\",\",\".\").astype(float)\n",
    "data[\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\"] = data[\"poor_mental_health_days_Average Number of Mentally Unhealthy Days\"].str.replace(\",\",\".\").astype(float)\n",
    "\n",
    "# Storing the merged, cleansed dataset as a csv file\n",
    "data.to_csv(\"Datasets/Mixed_data.csv\")\n",
    "merged.to_csv(\"Datasets/All_data.csv\")\n",
    "\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis.\n",
    "* Describe your data analysis and explain what you've learned about the dataset. *If relevant, talk about your machine-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre.\n",
    "* Which genre of data story did you use?\n",
    "* Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?\n",
    "* Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations.\n",
    "* Explain the visualizations you've chosen.\n",
    "* Why are they right for the story you want to tell?\n",
    "\n",
    "### Folium map\n",
    "\n",
    "### Choroploth Map\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think critically about your creation\n",
    "* What went well?\n",
    "* What is still missing? What could be improved? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
